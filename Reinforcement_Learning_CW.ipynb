{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nB1dhe1UvLGzrxAlDBad3fdY6m_uen_F",
      "authorship_tag": "ABX9TyMUFYHHsmLGaBG3GpdLLRtk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chickenlover32/RL_MyGitHub/blob/main/Reinforcement_Learning_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Coursework\n",
        "\n",
        "### Training and testing a PPO agent in Lunar Lander\n",
        "\n",
        "Swathi Suresh,\n",
        "CID: 02208023\n",
        "\n",
        "This notebook presents the implementation of a Proximal Policy Optimisation (PPO) agent in the LunarLander environment. It includes training, evaluation, and the generation of the required figures and visualisations.\n",
        "\n",
        "*   PPO Implementation : https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py\n",
        "*   Optimised Hyperparameters: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
        "\n",
        "*   Tutorial for implementation: https://github.com/Stable-Baselines-Team/rl-colab-notebooks?tab=readme-ov-file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "470L5uuJfLLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Imports"
      ],
      "metadata": {
        "id": "fSr4mo3Qvo_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart and remove variables\n",
        "%reset -f"
      ],
      "metadata": {
        "id": "7Bd0Th7Ozvym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive to save data\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VqMifRzlA7RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change directory on google drive\n",
        "%cd /content/drive/MyDrive/RL"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3DUwxGP0z1vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y swig ffmpeg\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install stable-baselines3\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w33BEJ_Q2FI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNQ86Q6Wv61W",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import gymnasium as gym            # Enviornment API\n",
        "from stable_baselines3 import PPO  # PPO algorithm\n",
        "\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "from datetime import datetime\n",
        "import tensorboard\n",
        "\n",
        "import base64\n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import ProgressBarCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "JFMSHr6tAVjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise directory to save logging info without overwrites\n",
        "\n",
        "run_id = 1\n",
        "log_dir = f\"/content/drive/MyDrive/RL/LunarLanderLogs{run_id}\"\n",
        "\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Set up logger\n",
        "new_logger = configure(log_dir, [\"stdout\", \"csv\", \"tensorboard\"])\n"
      ],
      "metadata": {
        "id": "IM7DK_tOwEM6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Lunar Lander Environment\n",
        "\n",
        "env = gym.make('LunarLander-v3', render_mode='human') # opens live rendering window\n",
        "env = Monitor(env, log_dir)                           # Save logs in log_dir/monitor.csv\n",
        "\n",
        "# Instantiate agent\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=1024,\n",
        "    batch_size=64,\n",
        "    n_epochs=4,\n",
        "    gamma=0.999,\n",
        "    gae_lambda=0.98,\n",
        "    clip_range=0.2,\n",
        "    clip_range_vf=None,\n",
        "    normalize_advantage=True,\n",
        "    ent_coef=0.01,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        "    use_sde=False,\n",
        "    tensorboard_log=log_dir,\n",
        "    verbose=2,\n",
        "    device=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "K0Y2MkBBwGJh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "\n",
        "model.set_logger(new_logger)\n",
        "model.learn(total_timesteps=1_000_000, callback=ProgressBarCallback())\n",
        "model.save(os.path.join(log_dir, \"PPO_lunarlanderv3\"))\n",
        "\n",
        "# env.close()\n"
      ],
      "metadata": {
        "id": "hEcQYHlBwIma",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hard coded save\n",
        "model.save(\"/content/drive/MyDrive/RL/LunarLanderLogs1/PPO_lunarlanderv3\")"
      ],
      "metadata": {
        "id": "ydGXOc4PmpZv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model\n"
      ],
      "metadata": {
        "id": "4YF3-JXjANo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise model for evaluation\n",
        "\n",
        "# Path to trained model\n",
        "log_dir = \"/content/drive/MyDrive/RL/LunarLanderLogs1\"    # CHANGE LUNAR LANDER LOGS IF NEEDED\n",
        "\n",
        "# Create evaluation environment\n",
        "env_Test = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "env_Test = Monitor(env_Test)\n",
        "\n",
        "# Load PPO model\n",
        "model = PPO.load(os.path.join(log_dir, \"PPO_lunarlanderv3.zip\"), env=env_Test) # CHANGE LUNAR LANDER LOGS IF NEEDED"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mkJpJ1_M9ja6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV file with logs\n",
        "\n",
        "#file_path = f\"{log_dir}/progress.csv\"\n",
        "file_path = \"/content/drive/MyDrive/RL/LunarLanderLogs1/progress.csv\"   # Hardcoded\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "K0DYzwNQwLpL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data.columns))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0_6rEqkvhz2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Plot Learning rate vs timesteps\n",
        "\n",
        "# Check required columns exist\n",
        "if (\"rollout/ep_rew_mean\" in data.columns) and (\"time/total_timesteps\" in data.columns):\n",
        "\n",
        "    plt.figure(\n",
        "        figsize=(10, 6),   # larger figure\n",
        "        dpi=150            # higher resolution\n",
        "    )\n",
        "\n",
        "    plt.plot(\n",
        "        data[\"time/total_timesteps\"],\n",
        "        data[\"rollout/ep_rew_mean\"],\n",
        "        color=\"blue\",\n",
        "        linewidth=2\n",
        "    )\n",
        "\n",
        "    plt.xlabel(\"Total Timesteps\", fontsize=12)\n",
        "    plt.ylabel(\"Average Mean Episode Reward\", fontsize=12)\n",
        "    plt.title(\"PPO on LunarLander: Mean Return vs Timesteps\", fontsize=14)\n",
        "\n",
        "    plt.grid(True, alpha=0.7)\n",
        "    plt.tight_layout(pad=3)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Required columns not found in progress.csv\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "veQuzL7qa_hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Distribution of rewards per episode\n",
        "\n",
        "# Load episodic rewards from monitor.csv\n",
        "monitor_path = os.path.join(log_dir, \"monitor.csv\")\n",
        "monitor_data = pd.read_csv(monitor_path, skiprows=1)  # skip header row\n",
        "\n",
        "# Episode rewards column is \"r\"\n",
        "episode_rewards = monitor_data[\"r\"]\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(8, 5), dpi=150)\n",
        "plt.hist(\n",
        "    episode_rewards,\n",
        "    bins=30,\n",
        "    color=\"steelblue\",\n",
        "    edgecolor=\"black\",\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Episode Reward\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"PPO on LunarLander: Distribution of Episode Rewards\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout(pad=3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kGqImLIczFS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display setup to record video\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "metadata": {
        "id": "BJriBmhPwBuy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for video recording\n",
        "def show_videos(video_path=\"\", prefix=\"\"):          # Shows videos with specific prefixes from certain folders\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "    :param video_path: (str) Path to the folder containing videos\n",
        "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "\n",
        "def record_video( model, env_id=\"LunarLander-v3\", video_folder=\"videos/\", prefix=\"ppo_eval\", video_length=1000 ):\n",
        "    \"\"\"\n",
        "    Record a video of a trained RL agent.\n",
        "\n",
        "    Args:\n",
        "        model: Trained RL model\n",
        "        env_id (str): Gym environment ID\n",
        "        video_folder (str): Directory to save videos\n",
        "        video_name_prefix (str): Prefix for video file names\n",
        "        video_length (int): Max number of steps to record\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    # Create environment\n",
        "    env = DummyVecEnv([ lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "\n",
        "    # Wrap with video recorder\n",
        "    env = VecVideoRecorder(\n",
        "        env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix\n",
        "    )\n",
        "\n",
        "    obs = env.reset()\n",
        "\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, _, _, _ = env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "QaYwWO_F9Fjz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record video for untrained agent\n",
        "\n",
        "untrained_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "untrained_model = PPO(policy=\"MlpPolicy\",env=untrained_env,verbose=0)\n",
        "\n",
        "record_video(\n",
        "    model=untrained_model,\n",
        "    env_id= \"LunarLander-v3\",\n",
        "    video_folder=\"/content/drive/MyDrive/RL/Videos\",\n",
        "    video_name_prefix=\"ppo_untrained_agent21e6\",\n",
        "    video_length=5000 # longer episode length\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SgoXJx2HjB3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record video for trained agent\n",
        "record_video(\n",
        "    model=model,\n",
        "    env_id=\"LunarLander-v3\",\n",
        "    video_folder=\"/content/drive/MyDrive/RL/videos\",\n",
        "    video_name_prefix=\"ppo_trained_agent_1e6\",\n",
        "    video_length=1000\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I36ODJLw9LWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Video\n",
        "show_videos(\"videos\", prefix=\"ppo\")"
      ],
      "metadata": {
        "id": "gvBVDYfF9SR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table to plot hyperparameter values and explanations\n",
        "data = {\n",
        "    \"Parameter\": [\n",
        "        \"Environment\",\n",
        "        \"Algorithm\",\n",
        "        \"Policy Network\",\n",
        "        \"Number of Environments (n_envs)\",\n",
        "        \"Total Timesteps (n_timesteps)\",\n",
        "        \"Rollout Steps (n_steps)\",\n",
        "        \"Batch Size (batch_size)\",\n",
        "        \"Discount Factor (gamma)\",\n",
        "        \"GAE Lambda (gae_lambda)\",\n",
        "        \"Number of Epochs (n_epochs)\",\n",
        "        \"Entropy Coefficient (ent_coef)\"\n",
        "    ],\n",
        "    \"Value\": [\n",
        "        \"LunarLander-v3\",\n",
        "        \"PPO\",\n",
        "        \"MlpPolicy\",\n",
        "        16,\n",
        "        1_000_000,\n",
        "        1024,\n",
        "        64,\n",
        "        0.999,\n",
        "        0.98,\n",
        "        4,\n",
        "        0.01\n",
        "    ],\n",
        "    \"Description\": [\n",
        "        \"OpenAI Gym environment for lunar landing control\",\n",
        "        \"Proximal Policy Optimization\",\n",
        "        \"Actor-Critic Policy\",\n",
        "        \"Number of environment copies running in parallel\",\n",
        "        \"Total number of environment interactions\",\n",
        "        \"Steps collected per environment before update\",\n",
        "        \"Minibatch size for PPO updates\",\n",
        "        \"Discount factor\",\n",
        "        \"Bias–variance trade-off parameter<br>for Generalised advantage function\",\n",
        "        \"Number of epoch when optimising the surrogate loss\",\n",
        "        \"Entropy coefficient for the loss calculation\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display LEFT-ALIGNED styled table (this must be the last line)\n",
        "df.style.set_properties(**{\"text-align\": \"left\"})\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JjBnGjBYzcuP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}